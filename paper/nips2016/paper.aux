\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{hastie2015statistical}
\citation{bach2012optimization}
\citation{kanwisher2010functional}
\citation{sporns14nn}
\citation{hubel1962receptive}
\citation{zeki1978functional}
\citation{iaria2008contrib}
\citation{buckner2013opportunities}
\citation{beckmann2005}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{daudet2004sparse}
\citation{harzallah2009combining}
\citation{kang2015structured}
\citation{vinci2014estimating}
\citation{kim2012tree}
\citation{jenatton2009structured}
\citation{jenatton2012multi}
\citation{jenatton2012multi}
\citation{bzdok2015semi}
\citation{doria2010}
\citation{anderson2013}
\citation{beckmann2005}
\citation{crad12}
\citation{smith2009}
\citation{crad12}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Rationale}{2}{subsection.2.1}}
\citation{jenatton2012multi}
\citation{yuan2006model}
\citation{bach2012optimization}
\citation{yuan2006model}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Problem formulation}{3}{subsection.2.2}}
\citation{beck2009}
\citation{smith2009}
\citation{crad12}
\citation{smith2009}
\citation{crad12}
\citation{abrah14}
\citation{pedr11}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Building blocks of the region-network tree.} Neurobiological priors introduced into the classification problem by hierarchical structured sparsity are displayed. \textit  {Left:} Continuous, partially overlapping brain network priors (\textit  {hot-colored}, taken from \cite  {smith2009}) accommodate the functional integration perspective of brain organization. \textit  {Right:} Discrete, non-overlapping brain region priors (\textit  {single-colored}, taken from \cite  {crad12}) accommodate the functional segregation perspective. \textit  {Middle:} These two types of predefined voxel groups are incorporated into a joint hierarchical prior of parent networks with their descending region child nodes. \textit  {Top to bottom:} Four examplary region-network priors are shown, including the early cortices that process visual and sound information from the environment, a well-known attentional circuit in the left brain hemisphere, and the cerebellum that is involved in motor behavior. }}{4}{figure.1}}
\newlabel{fig_priors}{{1}{4}{\textbf {Building blocks of the region-network tree.} Neurobiological priors introduced into the classification problem by hierarchical structured sparsity are displayed. \textit {Left:} Continuous, partially overlapping brain network priors (\textit {hot-colored}, taken from \cite {smith2009}) accommodate the functional integration perspective of brain organization. \textit {Right:} Discrete, non-overlapping brain region priors (\textit {single-colored}, taken from \cite {crad12}) accommodate the functional segregation perspective. \textit {Middle:} These two types of predefined voxel groups are incorporated into a joint hierarchical prior of parent networks with their descending region child nodes. \textit {Top to bottom:} Four examplary region-network priors are shown, including the early cortices that process visual and sound information from the environment, a well-known attentional circuit in the left brain hemisphere, and the cerebellum that is involved in motor behavior}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Hyperparameter optimization}{4}{subsection.2.3}}
\citation{barch2013}
\citation{harchaoui2012large}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Naive versus informed sparse model selection across training set sizes.} Ordinary $\ell _1$-penalized logistic regression (\textit  {upper row}) is compared to hierarchical-tree-penalized logistic regression ($\alpha = 1, \beta = 1$, \textit  {lower row}) with increasing fraction of the available training data to be fitted (\textit  {left to right columns}). For one example (i.e., ``View tools'') from 18 psychological tasks, unthresholded axial maps of model weights are shown for comparison against the sample average of that class (\textit  {rightmost column}, thresholded at the $75^{th}$ percentile). The support recovery of that exemplary class is given as Pearson correlation $r$ (cf. results). In the data-scarce scenario, ubiquitous in brain imaging studies, hierarchical tree sparsity achieves much better support recovery. In the data-rich scenario, neurobiologically informed logistic regression profits more from the available information quantities than neurobiologically naive logistic regression. }}{5}{figure.2}}
\newlabel{Tab:fig_dataratio}{{2}{5}{\textbf {Naive versus informed sparse model selection across training set sizes.} Ordinary $\ell _1$-penalized logistic regression (\textit {upper row}) is compared to hierarchical-tree-penalized logistic regression ($\alpha = 1, \beta = 1$, \textit {lower row}) with increasing fraction of the available training data to be fitted (\textit {left to right columns}). For one example (i.e., ``View tools'') from 18 psychological tasks, unthresholded axial maps of model weights are shown for comparison against the sample average of that class (\textit {rightmost column}, thresholded at the $75^{th}$ percentile). The support recovery of that exemplary class is given as Pearson correlation $r$ (cf. results). In the data-scarce scenario, ubiquitous in brain imaging studies, hierarchical tree sparsity achieves much better support recovery. In the data-rich scenario, neurobiologically informed logistic regression profits more from the available information quantities than neurobiologically naive logistic regression}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Implementation}{5}{subsection.2.4}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5}Data}{5}{subsection.2.5}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Results}{5}{section.3}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Benchmarking hierarchical tree sparsity against common sparsity penalties}{5}{subsection.3.1}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Prediction performance across sparsity priors.} Comparison of the performance of logistic regression estimators with 6 different structured and unstructered sparse regularization terms in classifying neural activity from 18 psychological tasks. The area under the curve (AUC, \textit  {colors}) is obtained on an identical test set as mean and class-wise measure. \textit  {Hierarchical Tree Sparsity}: Structured $\ell _1/\ell _2$-block norm with a hierarchy of region and network priors exhibited the best out-of-sample performance ($\alpha = 1, \beta = 1$). \textit  {Lasso}: Unstructured $\ell _1$-penalized logistic regression imposed a minimum of relevant brain voxels without assuming special structure. \textit  {Elastic-Net}: Unstructured logistic regression with interpolation between $\ell _1$- and $\ell _2$-norm imposed an equilibrium between sparsity and model fit. \textit  {(Sparse) Group Sparsity}: Structured $\ell _1/\ell _2$-block norm (with additional $\ell _1$ term) imposed region compartments, but naive to network structure. \textit  {Trace-norm}: Structured trace-norm penalization imposed low-rank structure with sparsity of unknown ``network'' patterns, but naive to region structure. A priori knowledge of both region and network neighborhoods was hence most beneficial for predicting psychological tasks from brain maps. }}{6}{figure.3}}
\newlabel{Tab:fig_sparsities}{{3}{6}{Experimental Results}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Sample complexity of naive versus informed sparse model selection}{6}{subsection.3.2}}
\citation{varoquaux2012small}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Support recovery as a function of region-network emphasis}{7}{subsection.3.3}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Support recovery as a function of region and network emphasis.} The relative impact of the region and network priors on model selection is systematically varied against each other (i.e., $\alpha $ and $\beta $ are changed reciprocally). Horizontal brain slices are shown with the voxel-wise weights for each class from the fitted predictive model. The region-network ratio (\textit  {columns}) weighted voxel groups to priviledge sparse models in function space that acknowledge known brain region neighborhoods (\textit  {left columns}) or known brain networks neighborhoods (\textit  {right columns}). Among the 18 classes, the model weights are shown for 5 exemplary psychological tasks followed by participants lying in a brain imaging scanner (\textit  {from top to bottom}): tongue movement, listening to stories, taking somebody else's perspective (ToM, "theory of mind"), as well as viewing locations and tools. The 18-class out-of-sample accuracy is indicated on the \textit  {bottom} and the class-wise mean neural activty (\textit  {rightmost column}, thresholded at the $75^{th}$ percentile). First, different emphasis on regions versus networks in hierarchical structured sparsity can yield very similar out-of-sample generalization. Second, favoring region versus network structure during model selection recovers complementary, non-identical aspects of the neural activity pattern underlying the psychological tasks. Third, equal emphasis on region versus network neighborhoods in the high-dimensional neuroimaging data yields more dispersed, less interpretable model choices. }}{8}{figure.4}}
\newlabel{fig_regnetratio}{{4}{8}{\textbf {Support recovery as a function of region and network emphasis.} The relative impact of the region and network priors on model selection is systematically varied against each other (i.e., $\alpha $ and $\beta $ are changed reciprocally). Horizontal brain slices are shown with the voxel-wise weights for each class from the fitted predictive model. The region-network ratio (\textit {columns}) weighted voxel groups to priviledge sparse models in function space that acknowledge known brain region neighborhoods (\textit {left columns}) or known brain networks neighborhoods (\textit {right columns}). Among the 18 classes, the model weights are shown for 5 exemplary psychological tasks followed by participants lying in a brain imaging scanner (\textit {from top to bottom}): tongue movement, listening to stories, taking somebody else's perspective (ToM, "theory of mind"), as well as viewing locations and tools. The 18-class out-of-sample accuracy is indicated on the \textit {bottom} and the class-wise mean neural activty (\textit {rightmost column}, thresholded at the $75^{th}$ percentile). First, different emphasis on regions versus networks in hierarchical structured sparsity can yield very similar out-of-sample generalization. Second, favoring region versus network structure during model selection recovers complementary, non-identical aspects of the neural activity pattern underlying the psychological tasks. Third, equal emphasis on region versus network neighborhoods in the high-dimensional neuroimaging data yields more dispersed, less interpretable model choices}{figure.4}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Out-of-sample performance by region-network emphasis}}{8}{table.1}}
\newlabel{table_reg_net_ratio}{{1}{8}{Out-of-sample performance by region-network emphasis}{table.1}{}}
\citation{passingham2002}
\citation{sporns14nn}
\citation{hastie2015statistical}
\citation{jenatton2009structured}
\citation{witten2010framework}
\citation{gramfort2011tracking}
\bibdata{../paper_refs}
\bibcite{abrah14}{1}
\bibcite{anderson2013}{2}
\bibcite{bach2012optimization}{3}
\bibcite{barch2013}{4}
\bibcite{beck2009}{5}
\@writefile{toc}{\contentsline {section}{\numberline {4}Conclusion}{9}{section.4}}
\bibcite{beckmann2005}{6}
\bibcite{buckner2013opportunities}{7}
\bibcite{bzdok2015semi}{8}
\bibcite{crad12}{9}
\bibcite{daudet2004sparse}{10}
\bibcite{doria2010}{11}
\bibcite{gramfort2011tracking}{12}
\bibcite{harchaoui2012large}{13}
\bibcite{harzallah2009combining}{14}
\bibcite{hastie2015statistical}{15}
\bibcite{hubel1962receptive}{16}
\bibcite{iaria2008contrib}{17}
\bibcite{jenatton2012multi}{18}
\bibcite{jenatton2009structured}{19}
\bibcite{kang2015structured}{20}
\bibcite{kanwisher2010functional}{21}
\bibcite{kim2012tree}{22}
\bibcite{passingham2002}{23}
\bibcite{pedr11}{24}
\bibcite{smith2009}{25}
\bibcite{sporns14nn}{26}
\bibcite{varoquaux2012small}{27}
\bibcite{vinci2014estimating}{28}
\bibcite{witten2010framework}{29}
\bibcite{yuan2006model}{30}
\bibcite{zeki1978functional}{31}
\bibstyle{splncs03}
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendix}{12}{section.5}}
\@writefile{toc}{\contentsline {paragraph}{Data.}{12}{section*.2}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Description of psychological tasks to predict.}}}{12}{table.2}}
\newlabel{table_tasks}{{2}{12}{\textbf {Description of psychological tasks to predict.}}{table.2}{}}

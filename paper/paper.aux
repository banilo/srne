\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{beckmann2005}
\citation{smith2009}
\citation{varoqu2011}
\citation{hipp15}
\citation{fox07}
\citation{fox07}
\citation{friston94}
\citation{friston97}
\citation{poldrack09decoding}
\citation{biswaldiscovery}
\citation{smith2009}
\citation{cole14}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\citation{hertz1991introduction}
\citation{hinton06}
\citation{baldi1989neural}
\citation{le2011ica}
\citation{olshausen96}
\citation{barch2013}
\citation{smith2013resting}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces \textbf  {Model architecture} Linear autoencoders find an optimized compression of 79,941 brain voxels into $n$ unknown activity patterns by improving reconstruction from them. The decomposition matrix equates with the bottleneck of a factored logistic regression. Supervised multi-class learning on task data ($X_{task}$) can thus be guided by unsupervised decomposition of rest data ($X_{rest}$). }}{2}{figure.1}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Methods}{2}{section.2}}
\@writefile{toc}{\contentsline {paragraph}{Data.}{2}{section*.1}}
\citation{pinel07}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces \textbf  {Description of psychological tasks to predict.}}}{3}{table.1}}
\newlabel{table_tasks}{{1}{3}{\textbf {Description of psychological tasks to predict.}}{table.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Linear autoencoder.}{3}{section*.2}}
\newlabel{eq:autoenc}{{1}{3}{Linear autoencoder}{equation.2.1}{}}
\@writefile{toc}{\contentsline {paragraph}{Factored logistic regression.}{3}{section*.3}}
\citation{rmsprop}
\citation{abrah14}
\citation{bastien2012theano}
\citation{bergstra2010theano}
\newlabel{eq:lr}{{3}{4}{Factored logistic regression}{equation.2.3}{}}
\newlabel{eq:lr_loss}{{4}{4}{Factored logistic regression}{equation.2.4}{}}
\@writefile{toc}{\contentsline {paragraph}{Layer combination.}{4}{section*.4}}
\newlabel{eq:loss_equ}{{6}{4}{Layer combination}{equation.2.6}{}}
\@writefile{toc}{\contentsline {paragraph}{Optimization.}{4}{section*.5}}
\@writefile{toc}{\contentsline {paragraph}{Implementation.}{4}{section*.6}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Experimental Results}{5}{section.3}}
\@writefile{toc}{\contentsline {paragraph}{Serial versus parallel structure discovery and classification.}{5}{section*.7}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces \textbf  {Serial versus parallel dimensionality reduction and classification.} Chance is at 5,6\%. }}{5}{table.2}}
\newlabel{table_one}{{2}{5}{\textbf {Serial versus parallel dimensionality reduction and classification.} Chance is at 5,6\%}{table.2}{}}
\@writefile{toc}{\contentsline {paragraph}{Model performance.}{5}{section*.8}}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces \textbf  {Performance of SSFLogReg across model parameter choices.} Chance is at 5.6\%. }}{5}{table.3}}
\newlabel{table_two}{{3}{5}{\textbf {Performance of SSFLogReg across model parameter choices.} Chance is at 5.6\%}{table.3}{}}
\citation{barch2013}
\citation{barch2013}
\citation{barch2013}
\@writefile{toc}{\contentsline {paragraph}{Individual effects of dimensionality reduction and rest data.}{6}{section*.9}}
\@writefile{toc}{\contentsline {paragraph}{Feature identification.}{6}{section*.10}}
\@writefile{toc}{\contentsline {paragraph}{Miscellaneous observations.}{6}{section*.11}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Discussion and Conclusion}{6}{section.4}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces \textbf  {Effect of bottleneck in a 38-task classificaton problem} Depicts the f1 prediction scores for each of 38 psychological tasks. Multinomial logistic regression operating in voxel space (\textit  {blue bars}) was compared to SSFLogReg operating in $20$ (\textit  {left plot}) and $100$ (\textit  {right plot}) latent modes (\textit  {grey bars}). Autoencoder or rest data were not used for these analyses ($\lambda =1$). Ordinary logistic regression yielded 77.7\% accuracy out of sample, while SSFLogReg scored at 94.4\% ($n=20$) and 94.2\% ($n=100$). Hence, compressing the voxel data into a component space for classification achieves higher task separability. Chance is at $2,6\%$. }}{7}{figure.2}}
\newlabel{fig_dimred}{{2}{7}{\textbf {Effect of bottleneck in a 38-task classificaton problem} Depicts the f1 prediction scores for each of 38 psychological tasks. Multinomial logistic regression operating in voxel space (\textit {blue bars}) was compared to SSFLogReg operating in $20$ (\textit {left plot}) and $100$ (\textit {right plot}) latent modes (\textit {grey bars}). Autoencoder or rest data were not used for these analyses ($\lambda =1$). Ordinary logistic regression yielded 77.7\% accuracy out of sample, while SSFLogReg scored at 94.4\% ($n=20$) and 94.2\% ($n=100$). Hence, compressing the voxel data into a component space for classification achieves higher task separability. Chance is at $2,6\%$}{figure.2}{}}
\citation{poldrack2014data}
\citation{schwartz2013mapping}
\citation{amunts2013bigbrain}
\citation{need2010gwas}
\citation{frackowiak2015future}
\bibstyle{splncs03}
\bibdata{paper_refs}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces \textbf  {Effect of rest structure} Model performance of SSFLogReg ($n=20$, $\ell _1=0.1$, $\ell _2=0.1$) for different choices of $\lambda $ in data-scarce (100 task and 100 rest maps, \textit  {hot color}) and data-rich (1000 task and 1000 rest maps, \textit  {cold color}) scenarios. Gradient descent was performed on 2000 task and 2000 rest maps. At the begining of each epoch, these were drawn with replacement from a pool of 100 or 1000 different task and rest maps, respectively. Chance is at 5.6\%. }}{8}{figure.3}}
\newlabel{fig_semisup}{{3}{8}{Individual effects of dimensionality reduction and rest data}{figure.3}{}}
\@writefile{toc}{\contentsline {paragraph}{Acknowledgment}{8}{section*.12}}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces \textbf  {Classification weight maps} The voxel predictors corresponding to 5 exemplary (of 18 total) psychological tasks (\textit  {rows}) from the HCP dataset \cite  {barch2013}. \textit  {Left column:} multinomial logistic regression (same implementation but without bottleneck or autoencoder), \textit  {middle column:} SSFLogReg ($n=20$ latent components, $\lambda =0.5$, $\ell _1=0.1$, $\ell _2=0.1$), \textit  {right column:} voxel-wise average across all samples of whole-brain activity maps from each task. SSFLogReg a) puts higher absolute weights on relevant structure, b) lower ones on irrelevant structure, and c) yields BOLD-typical local contiguity (without enforcing an explicit spatial prior). All values are z-scored and thresholded at the $75^{th}$ percentile. }}{9}{figure.4}}
\newlabel{fig_weights}{{4}{9}{\textbf {Classification weight maps} The voxel predictors corresponding to 5 exemplary (of 18 total) psychological tasks (\textit {rows}) from the HCP dataset \cite {barch2013}. \textit {Left column:} multinomial logistic regression (same implementation but without bottleneck or autoencoder), \textit {middle column:} SSFLogReg ($n=20$ latent components, $\lambda =0.5$, $\ell _1=0.1$, $\ell _2=0.1$), \textit {right column:} voxel-wise average across all samples of whole-brain activity maps from each task. SSFLogReg a) puts higher absolute weights on relevant structure, b) lower ones on irrelevant structure, and c) yields BOLD-typical local contiguity (without enforcing an explicit spatial prior). All values are z-scored and thresholded at the $75^{th}$ percentile}{figure.4}{}}
\citation{barch2013}
\citation{barch2013}
\citation{smith2013resting}
\citation{pinel07}
\citation{gorgo11}
\@writefile{toc}{\contentsline {section}{\numberline {5}Appendix}{10}{section.5}}
\@writefile{toc}{\contentsline {paragraph}{Data.}{10}{section*.13}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces \textbf  {Weight maps of a same hidden factor ranging from unsupervised to supervised regime} One of the $n$ factors from the hidden layer ($\mathbf  {W_0}$) was plotted for the same data (full HCP dataset) and the same model choices ($n=20, \ell _1=0.1$, $\ell _2=0.1$) along a $\lambda $-grid between purely unsupervised ($\lambda =0.0$, \textit  {top row}) and purely supervised ($\lambda =1.0$, \textit  {bottom row}) settings. As qualitative evidence, a slow transition from rest- to task-typical brain networks was observed in brain space. Although difficult to quantify, rest network elements appear to get 'reassembled' to latent factors of the LR. This increased confidence that the improved model performance of rest-informed fLR is not only an arbitrary effect of spatially smooth noise. All values are z-scored. }}{11}{figure.5}}
\newlabel{fig_weights_conversion}{{5}{11}{\textbf {Weight maps of a same hidden factor ranging from unsupervised to supervised regime} One of the $n$ factors from the hidden layer ($\mathbf {W_0}$) was plotted for the same data (full HCP dataset) and the same model choices ($n=20, \ell _1=0.1$, $\ell _2=0.1$) along a $\lambda $-grid between purely unsupervised ($\lambda =0.0$, \textit {top row}) and purely supervised ($\lambda =1.0$, \textit {bottom row}) settings. As qualitative evidence, a slow transition from rest- to task-typical brain networks was observed in brain space. Although difficult to quantify, rest network elements appear to get 'reassembled' to latent factors of the LR. This increased confidence that the improved model performance of rest-informed fLR is not only an arbitrary effect of spatially smooth noise. All values are z-scored}{figure.5}{}}
